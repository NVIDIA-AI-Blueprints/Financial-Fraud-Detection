{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The objective of this notebook is to showcase the usage of the ___financial-fraud-training___ NIM (microservice) (NEED LINK) and how to deploy the produced trained models on the Triton Inference Server.\n",
    "- We use [IBM TabFromer](https://github.com/IBM/TabFormer) as an example dataset\n",
    "- That datset is then preprocess before running through the training NIM.\n",
    "\n",
    "NOTICE:\n",
    "- This notebook assume that you have followed the pre\n",
    "\n",
    "NOTE: The preprocessing code is written specifically for the TabFormer dataset and will not work with other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 1: Get and Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Unfortunatley the data need to be downloaded manually___\n",
    "\n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. untar and uncompreess the file: `tar -xvzf ./transactions.tgz`\n",
    "3. Put card_transaction.v1.csv in in the `TabFormer/raw` folder\n",
    "\n",
    "The goal is to produce the following structure\n",
    "\n",
    "```\n",
    ".\n",
    "    data\n",
    "    └── TabFormer\n",
    "        └── raw\n",
    "            └── card_transaction.v1.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the raw data is placed as described above, set the path to the TabFormer directory\n",
    "\n",
    "# Change this path to point to TabFormer data\n",
    "data_root_dir = os.path.abspath('../data/TabFormer/') \n",
    "\n",
    "# Change this path to the directory where you want to save your model\n",
    "model_output_dir = os.path.join(data_root_dir, 'trained_models')\n",
    "\n",
    "\n",
    "# Path to save the trained model\n",
    "os.makedirs(model_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(directory, prefix=\"\"):\n",
    "    \"\"\"Recursively prints the directory tree starting at 'directory'.\"\"\"\n",
    "    # Retrieve a sorted list of entries in the directory\n",
    "    entries = sorted(os.listdir(directory))\n",
    "    entries_count = len(entries)\n",
    "    \n",
    "    for index, entry in enumerate(entries):\n",
    "        path = os.path.join(directory, entry)\n",
    "        # Determine the branch connector\n",
    "        if index == entries_count - 1:\n",
    "            connector = \"└── \"\n",
    "            extension = \"    \"\n",
    "        else:\n",
    "            connector = \"├── \"\n",
    "            extension = \"│   \"\n",
    "        \n",
    "        print(prefix + connector + entry)\n",
    "        \n",
    "        # If the entry is a directory, recursively print its contents\n",
    "        if os.path.isdir(path):\n",
    "            print_tree(path, prefix + extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the raw data has been placed properly\n",
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Preprocess the data \n",
    "- Import the Python function for preprocessing the TabFormer data\n",
    "- Call `preprocess_TabFormer` function to prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the \"src\" directory to the search path\n",
    "src_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "# should be able to import from \"src\" folder now\n",
    "from preprocess_TabFormer import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "mask_mapping, feature_mask = preprocess_data(data_root_dir)\n",
    "\n",
    "# this will output status as it correlates different attributes with target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should not see files under a \"gnn\" folder and under a \"xgb\" folder\n",
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Step 3:  Now train the model using the financial-fraud-training NIM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training configuration file\n",
    "NOTE: Training configuration file must conform to the training schemas defined in financial-fraud-training NIM  (NOTE:  NEED A LINK TO THE DOCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important: Models and configuration files needed for deployment using the Triton Inference server will be saved in model-repository under the folder that is mounted in /trained_models inside the NIM container__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "  \"paths\": {\n",
    "    \"data_dir\": \"/data\", # Mount dataset root directory under /data in the container\n",
    "    \"output_dir\": \"/trained_models\" # Mount path to save the trained models.\n",
    "                                    # NOTE: This path is inside the docker container \n",
    "  },\n",
    "\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"kind\": \"GraphSAGE_XGBoost\",\n",
    "      \"gpu\": \"single\",\n",
    "      \"hyperparameters\": {\n",
    "        \"gnn\":{\n",
    "          \"hidden_channels\": 16,\n",
    "          \"n_hops\": 1,\n",
    "          \"dropout_prob\": 0.1,\n",
    "          \"batch_size\": 1024,\n",
    "          \"fan_out\": 16,\n",
    "          \"num_epochs\": 16\n",
    "        },\n",
    "        \"xgb\": {\n",
    "          \"max_depth\": 6,\n",
    "          \"learning_rate\": 0.2,\n",
    "          \"num_parallel_tree\": 3,\n",
    "          \"num_boost_round\": 512,\n",
    "          \"gamma\": 0.0\n",
    "        }\n",
    "\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the training configuration file as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config_file_name = 'training_config.json'\n",
    "\n",
    "with open(os.path.join(training_config_file_name), 'w') as json_file:\n",
    "    json.dump(training_config, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull and run the financial_fraud_training NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=os.environ.get('NGC_API_KEY')\n",
    "## NEED TO PULL CONTAINER ONCE IT IS IN NGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login and pull the image from the NGC registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker login nvcr.io --username '$oauthtoken' --password {API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull nvcr.io/nvstaging/nim/financial-fraud-training:1.0.0-rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a local cache directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = subprocess.run([\"whoami\"], capture_output=True, text=True).stdout.strip()\n",
    "nim_cache_dir = f'/home/{username}/.cache/nim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {nim_cache_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set container name and ports for running the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIM_HTTP_PORT = 8002\n",
    "NIM_GRPC_PORT = 50051\n",
    "CONTAINER_NAME = \"financial-fraud-training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any running container with the same name\n",
    "!docker stop {CONTAINER_NAME}\n",
    "!docker rm {CONTAINER_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -it --rm --name={CONTAINER_NAME} --gpus all \\\n",
    "    -p {NIM_HTTP_PORT}:{NIM_HTTP_PORT} -e NIM_HTTP_API_PORT={NIM_HTTP_PORT} -p {NIM_GRPC_PORT}:{NIM_GRPC_PORT} \\\n",
    "    -e NIM_DISABLE_MODEL_DOWNLOAD=True -e NIM_GRPC_API_PORT={NIM_GRPC_PORT} -e NIM_CACHE_PATH=/opt/nim/.cache \\\n",
    "    -e NIM_CACHE_PATH=/opt/nim/.cache  --mount=type=bind,src={nim_cache_dir},dst=/opt/nim/.cache -v {data_root_dir}:/data \\\n",
    "    -v {model_output_dir}:/trained_models nvcr.io/nvstaging/nim/financial-fraud-training:1.0.0-rc1 -e NGC_API_KEY={API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, initiate model training using the training configuration defined earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initiate training via the /train endpoint by sending the training configuration as a JSON payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmd = [\n",
    "    \"curl\",\n",
    "    \"-X\", \"POST\",\n",
    "    \"-H\", \"Content-Type: application/json\",\n",
    "    \"-d\", json.dumps(training_config),\n",
    "    f\"http://0.0.0.0:{NIM_HTTP_PORT}/train\"\n",
    "]\n",
    "# result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "# result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://0.0.0.0:$NIM_HTTP_PORT/train\"   -H \"Content-Type: application/json\"   -d @{training_config_file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that the `model_repository` has been created with right contents in it\n",
    "According the above defined configuration file, the `model_repository`, which is folder containing the models and configuration files to be deployed on the Triton inference Server, will be created under \n",
    "{data_root_dir}/trained_models/ and its contents will look like\n",
    "\n",
    "```sh\n",
    "├── model\n",
    "│   ├── 1\n",
    "│   │   └── graph_sage_node_embedder.onnx\n",
    "│   └── config.pbtxt\n",
    "└── xgboost\n",
    "    ├── 1\n",
    "    │   └── xgboost_on_embeddings.json\n",
    "    └── config.pbtxt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(os.path.join(model_output_dir, 'model_repository'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 3:  Serve your model on Triton Inference Server\n",
    "\n",
    "!Important: Change MODEL_REPO_PATH to point to the `model repository` folder if you used different path in your training configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install tritonclient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient import utils as triton_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to False for remote/cloud deployment\n",
    "run_locally = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace HOST with the actual server URL where your Triton Inference Server is hosted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_locally:\n",
    "    HOST = 'localhost'\n",
    "else:\n",
    "    HOST = '<SERVER_URL>' # Replace with your server URL or IP address\n",
    "\n",
    "TRITON_HTTP_PORT = 8000\n",
    "TRITON_GRPC_PORT = 8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are testing a local deployment\n",
    "- Pull Triton inference server docker image\n",
    "- Deploy server with  models and configuration files (produced by the training NIM)\n",
    "- Double check that your model repository folder has the following structures\n",
    "```sh\n",
    "├── model\n",
    "│   ├── 1\n",
    "│   │   └── graph_sage_node_embedder.onnx\n",
    "│   └── config.pbtxt\n",
    "└── xgboost\n",
    "    ├── 1\n",
    "    │   └── xgboost_on_embeddings.json\n",
    "    └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_locally:\n",
    "    \n",
    "    # Triton server image\n",
    "    TRITON_IMAGE = 'nvcr.io/nvidia/tritonserver:25.01-py3'\n",
    "    MODEL_REPO_PATH = os.path.join(model_output_dir, 'model_repository')\n",
    "\n",
    "    # Pull docker \n",
    "    !docker pull {TRITON_IMAGE}\n",
    "    !docker stop tritonserver\n",
    "    !docker rm tritonserver\n",
    "\n",
    "    !docker run --gpus all -d -p {TRITON_HTTP_PORT}:{TRITON_HTTP_PORT} -p {TRITON_GRPC_PORT}:{TRITON_GRPC_PORT} -v {MODEL_REPO_PATH}:/models --name tritonserver {TRITON_IMAGE} tritonserver --model-repository=/models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs for GRPC and HTTP request to the inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{TRITON_GRPC_PORT}')\n",
    "client_http = httpclient.InferenceServerClient(url=f'{HOST}:{TRITON_HTTP_PORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the triton inference server to come online\n",
    "NOTE: If the following cell keeps running longer then interrupt execution and run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TIMEOUT = 60\n",
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{TRITON_GRPC_PORT}')\n",
    "server_start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        if client_grpc.is_server_ready() or time.time() - server_start > TIMEOUT:\n",
    "            break\n",
    "    except triton_utils.InferenceServerException:\n",
    "        pass\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For local deployment, check if the triton inference server is running properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_locally:\n",
    "    !docker logs tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read preprocessed input transactions to make query to the triton inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_path = os.path.join(data_root_dir, \"xgb/test.csv\") # already preprocessed data\n",
    "test_df = pd.read_csv(test_path)\n",
    "X = test_df.iloc[:, :-1].values.astype(np.float32)\n",
    "y = test_df.iloc[:, -1].values\n",
    "edge_index = np.array([[], []]).astype(np.int64) # empty edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the HTTP request's inputs and output to retrieve embeddings for the input transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = httpclient.InferInput(\"x\", X.shape, datatype=\"FP32\")\n",
    "input_features.set_data_from_numpy(X)\n",
    "\n",
    "input_edge_indices = httpclient.InferInput(\"edge_index\", edge_index.shape, datatype=\"INT64\")\n",
    "input_edge_indices.set_data_from_numpy(edge_index)\n",
    "\n",
    "outputs = httpclient.InferRequestedOutput(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a query to retrieve embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the server\n",
    "results = client_http.infer(model_name=\"model\", inputs=[input_features, input_edge_indices], outputs=[outputs])\n",
    "node_embeddings = results.as_numpy('output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the retrieved embeddings as inputs to predict the transactions' fraud scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_input = httpclient.InferInput(\"input__0\", node_embeddings.shape, datatype=\"FP32\")\n",
    "xgboost_input.set_data_from_numpy(node_embeddings)\n",
    "\n",
    "xgboost_outputs = httpclient.InferRequestedOutput(\"output__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a query to retrieve the fraud scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client_http.infer(model_name=\"xgboost\", inputs=[xgboost_input], outputs=[xgboost_outputs])\n",
    "predictions = results.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision threshold to flag a transaction as fraud\n",
    "#Change to trade-off precision and recall\n",
    "decision_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = (predictions > decision_threshold).astype(int)\n",
    "\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred, zero_division=0)\n",
    "recall = recall_score(y, y_pred, zero_division=0)\n",
    "f1 = f1_score(y, y_pred, zero_division=0)\n",
    "\n",
    "print(\"----Summary---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create a DataFrame with labeled rows and columns\n",
    "classes = ['Non-Fraud', 'Fraud']\n",
    "columns = pd.MultiIndex.from_product([[\"Predicted\"], classes])\n",
    "index = pd.MultiIndex.from_product([[\"Actual\"], classes])\n",
    "\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "cm_df = pd.DataFrame(conf_mat, index=index, columns=columns)\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Plot the confusion matrix directly from predictions\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    y, y_pred, display_labels=classes)\n",
    "disp.ax_.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 4:  Serve your python backend model on Triton Inference Server\n",
    "\n",
    "!Important: Change MODEL_REPO_PATH to point to the `model python_backend_model_repository` folder if you used different path in your training configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy python backend model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_locally:\n",
    "    HOST = 'localhost'\n",
    "else:\n",
    "    HOST = '<SERVER_URL>' # Replace with your server URL or IP address\n",
    "\n",
    "HTTP_PORT = 8005\n",
    "GRPC_PORT = 8006\n",
    "METRICS_PORT = 8007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_locally:\n",
    "    \n",
    "    # Triton server image\n",
    "    TRITON_IMAGE = 'nvcr.io/nvidia/tritonserver:25.01-py3'\n",
    "    MODEL_REPO_PATH = os.path.join(model_output_dir, 'python_backend_model_repository')\n",
    "\n",
    "    # Pull docker \n",
    "    !docker pull {TRITON_IMAGE}\n",
    "    !docker stop tritonserver\n",
    "    !docker rm tritonserver\n",
    "\n",
    "    !docker run --gpus all -d -p {HTTP_PORT}:{HTTP_PORT} -p {GRPC_PORT}:{GRPC_PORT} \\\n",
    "        -v {MODEL_REPO_PATH}:/models --name tritonserver {TRITON_IMAGE} tritonserver \\\n",
    "        --model-repository=/models   --http-port={HTTP_PORT} --grpc-port={GRPC_PORT} \\\n",
    "        --metrics-port={METRICS_PORT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "container_name = \"tritonserver\"\n",
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        if client_grpc.is_server_ready():\n",
    "            break\n",
    "    except triton_utils.InferenceServerException as e:\n",
    "        pass\n",
    "    try:\n",
    "        # Run the docker logs command with the --tail option\n",
    "        output = subprocess.check_output([\"docker\", \"logs\", \"--tail\", \"10\", container_name])\n",
    "        print(output.decode(\"utf-8\"))\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error retrieving logs:\", e)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction without computing Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"prediction_and_shapley\"\n",
    "test_path = os.path.join(data_root_dir, \"xgb/test.csv\") # already preprocessed data\n",
    "test_df = pd.read_csv(test_path)\n",
    "X = test_df.iloc[:, :-1].values.astype(np.float32)\n",
    "y = test_df.iloc[:, -1].values\n",
    "edge_index = np.array([[], []]).astype(np.int64) # empty edge_index\n",
    "compute_shap = np.array([False], dtype=bool) # Skip shap value computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mask = feature_mask.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with httpclient.InferenceServerClient(f\"localhost:{HTTP_PORT}\") as client:\n",
    "    input_features = httpclient.InferInput(\"NODE_FEATURES\", X.shape, datatype=\"FP32\")\n",
    "    input_features.set_data_from_numpy(X)\n",
    "\n",
    "    input_edge_indices = httpclient.InferInput(\"EDGE_INDEX\", edge_index.shape, datatype=\"INT64\")\n",
    "    input_edge_indices.set_data_from_numpy(edge_index)\n",
    "\n",
    "    input_feature_mask = httpclient.InferInput(\"FEATURE_MASK\", feature_mask.shape, datatype=\"INT32\")\n",
    "    input_feature_mask.set_data_from_numpy(feature_mask)\n",
    "\n",
    "    compute_shap_flag = httpclient.InferInput(\"COMPUTE_SHAP\", compute_shap.shape, datatype=\"BOOL\")\n",
    "    compute_shap_flag.set_data_from_numpy(compute_shap)\n",
    "    \n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"PREDICTION\"),\n",
    "        httpclient.InferRequestedOutput(\"SHAP_VALUES\")\n",
    "    ]\n",
    "    response = client.infer(model_name, inputs=[input_features, input_edge_indices, compute_shap_flag, input_feature_mask ], request_id=str(1), outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = response.as_numpy('PREDICTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = (predictions > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred, zero_division=0)\n",
    "recall = recall_score(y, y_pred, zero_division=0)\n",
    "f1 = f1_score(y, y_pred, zero_division=0)\n",
    "\n",
    "print(\"----Summary---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create a DataFrame with labeled rows and columns\n",
    "classes = ['Non-Fraud', 'Fraud']\n",
    "columns = pd.MultiIndex.from_product([[\"Predicted\"], classes])\n",
    "index = pd.MultiIndex.from_product([[\"Actual\"], classes])\n",
    "\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "cm_df = pd.DataFrame(conf_mat, index=index, columns=columns)\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Plot the confusion matrix directly from predictions\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    y, y_pred, display_labels=classes)\n",
    "disp.ax_.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Shapley value for different features for a transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set COMPUTE_SHAP flag to True\n",
    "compute_shap = np.array([True], dtype=bool)\n",
    "\n",
    "X = test_df.iloc[:1, :-1].values.astype(np.float32)\n",
    "y = test_df.iloc[:1, -1].values\n",
    "\n",
    "with httpclient.InferenceServerClient(f\"localhost:{HTTP_PORT}\") as client:\n",
    "    input_features = httpclient.InferInput(\"NODE_FEATURES\", X.shape, datatype=\"FP32\")\n",
    "    input_features.set_data_from_numpy(X)\n",
    "\n",
    "    input_edge_indices = httpclient.InferInput(\"EDGE_INDEX\", edge_index.shape, datatype=\"INT64\")\n",
    "    input_edge_indices.set_data_from_numpy(edge_index)\n",
    "\n",
    "    input_feature_mask = httpclient.InferInput(\"FEATURE_MASK\", feature_mask.shape, datatype=\"INT32\")\n",
    "    input_feature_mask.set_data_from_numpy(feature_mask)\n",
    "\n",
    "    compute_shap_flag = httpclient.InferInput(\"COMPUTE_SHAP\", compute_shap.shape, datatype=\"BOOL\")\n",
    "    compute_shap_flag.set_data_from_numpy(compute_shap)\n",
    "    \n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"PREDICTION\"),\n",
    "        httpclient.InferRequestedOutput(\"SHAP_VALUES\")\n",
    "    ]\n",
    "    response = client.infer(model_name, inputs=[input_features, input_edge_indices, compute_shap_flag, input_feature_mask ], request_id=str(1), outputs=outputs)\n",
    "\n",
    "\n",
    "predictions= response.as_numpy('PREDICTION')\n",
    "shap_values = response.as_numpy('SHAP_VALUES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_attribution_map = dict(zip(feature_mask, shap_values[0]))\n",
    "feature_name_to_id_map = {v:k  for k,v in mask_mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapley values for different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{feature_name_to_id_map[k]: f\"{v:.3f}\" for k, v in feature_to_attribution_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_env_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
