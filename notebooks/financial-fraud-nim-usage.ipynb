{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the \"src\" directory to the search path\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be able to import from \"src\" folder now\n",
    "from preprocess_TabFormer import proprocess_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 1: Get and Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Unfortunatley the data need to be downloaded manually___\n",
    "\n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. untar and uncompreess the file: tar -xvzf ./transactions.tgz\n",
    "3. Put card_transaction.v1.csv in in the \"./data/TabFormer/raw\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to your dataset root directory\n",
    "dataset_root_dir = '../data/TabFormer/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to produce the following structure\n",
    "```sh\n",
    "data/TabFormer \n",
    "├── raw\n",
    "│   ├── card_transaction.v1.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../data/TabFormer/\u001b[0m\n",
      "└── \u001b[01;34mraw\u001b[0m\n",
      "    ├── card_transaction.v1.csv\n",
      "    ├── \u001b[01;34mgnn\u001b[0m\n",
      "    └── \u001b[01;34mxgb\u001b[0m\n",
      "\n",
      "3 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "!tree {dataset_root_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation (Card, Fraud) =   6.59%\n",
      "Correlation (Chip, Fraud) =   5.63%\n",
      "Correlation (Errors, Fraud) =   1.81%\n",
      "Correlation (State, Fraud) =  35.92%\n",
      "Correlation (City, Fraud) =  32.47%\n",
      "Correlation (Zip, Fraud) =  14.99%\n",
      "Correlation (MCC, Fraud) =  12.70%\n",
      "Correlation (Merchant, Fraud) =  34.88%\n",
      "Correlation (User, Fraud) =   3.40%\n",
      "Correlation (Day, Fraud) =   0.26%\n",
      "Correlation (Month, Fraud) =   0.23%\n",
      "Correlation (Year, Fraud) =   2.35%\n",
      "r_pb (Time) = -0.00 with p_value 0.00\n",
      "r_pb (Amount) = 0.03 with p_value 0.00\n",
      "Transaction ID range (0, 281107)\n",
      "Merchant ID range (281108, 322375)\n",
      "User ID range (322376, 327149)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "proprocess_data(dataset_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../data/TabFormer/\u001b[0m\n",
      "├── \u001b[01;34mgnn\u001b[0m\n",
      "│   ├── edges.csv\n",
      "│   ├── features.csv\n",
      "│   ├── info.json\n",
      "│   └── labels.csv\n",
      "├── \u001b[01;34mraw\u001b[0m\n",
      "│   ├── card_transaction.v1.csv\n",
      "│   ├── \u001b[01;34mgnn\u001b[0m\n",
      "│   └── \u001b[01;34mxgb\u001b[0m\n",
      "└── \u001b[01;34mxgb\u001b[0m\n",
      "    ├── example_transactions.csv\n",
      "    ├── test.csv\n",
      "    ├── training.csv\n",
      "    ├── untransformed_test.csv\n",
      "    └── validation.csv\n",
      "\n",
      "5 directories, 10 files\n"
     ]
    }
   ],
   "source": [
    "!tree {dataset_root_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Preprocess data and run training using Training NIM\n",
    "- Call function to preprocess data\n",
    "- Train models using Training NIM\n",
    "  - - For local testing, deploy Training NIM\n",
    "  - - Train model based on input config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training configuration file\n",
    "NOTE: Training configuration file must conform to the training schemas defined in Training NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the trained model\n",
    "os.makedirs(os.path.join(dataset_root_dir, 'trained_models'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !Important: Models and configuration files needed to deploy using Triton Inference server will be saved in trained_models/model-repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "  \"paths\": {\n",
    "    \"data_dir\": \"/data\", # Mound dataset root directory under /data in the container\n",
    "    \"output_dir\": \"/data/trained_models\" # Mounted path to save the trained models\n",
    "  },\n",
    "\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"kind\": \"GraphSAGE_XGBoost\",\n",
    "      \"gpu\": \"single\",\n",
    "      \"hyperparameters\": {\n",
    "        \"gnn\":{\n",
    "          \"hidden_channels\": 16,\n",
    "          \"n_hops\": 1,\n",
    "          \"dropout_prob\": 0.1,\n",
    "          \"batch_size\": 1024,\n",
    "          \"fan_out\": 16,\n",
    "          \"num_epochs\": 16\n",
    "        },\n",
    "        \"xgb\": {\n",
    "          \"max_depth\": 6,\n",
    "          \"learning_rate\": 0.2,\n",
    "          \"num_parallel_tree\": 3,\n",
    "          \"num_boost_round\": 512,\n",
    "          \"gamma\": 0.0\n",
    "        }\n",
    "\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the training configuration file as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "training_config_file_name = 'training_config.json'\n",
    "\n",
    "with open(os.path.join(training_config_file_name), 'w') as json_file:\n",
    "    json.dump(training_config, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure to pull the training container or build from the source using\n",
    "```sh\n",
    "    docker build --no-cache -t training_container /path/to/training_NIM_repo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally train the models according to above defined configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Head \"http://%2Fvar%2Frun%2Fdocker.sock/_ping\": dial unix /var/run/docker.sock: connect: permission denied.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run --cap-add SYS_NICE -it --rm  --gpus all  -v {dataset_root_dir}:/data -v ./{training_config_file_name}:/app/config.json training_container --config /app/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that the `model_repository` has been created with right contents in it\n",
    "According the above defined configuration file, the `model_repository`, which is folder containing the models and configuration files to be deployed on the Triton inference Server, will be created under \n",
    "{dataset_root_dir}/trained_models/ and its contents will look like\n",
    "\n",
    "```sh\n",
    "├── model\n",
    "│   ├── 1\n",
    "│   │   └── graph_sage_node_embedder.onnx\n",
    "│   └── config.pbtxt\n",
    "└── xgboost\n",
    "    ├── 1\n",
    "    │   └── xgboost_on_embeddings.json\n",
    "    └── config.pbtxt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/TabFormer//trained_models/model_repository  [error opening dir]\n",
      "\n",
      "0 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!tree {dataset_root_dir}/trained_models/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 3:  Serve your model on Triton Inference Server\n",
    "\n",
    "!Important: Change MODEL_REPO_PATH to point to the `model repository` folder if you used different path in your training configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install tritonclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tritonclient[all]\n",
      "  Downloading tritonclient-2.54.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.19.1 in /home/brad/miniforge3/envs/notebook_env/lib/python3.12/site-packages (from tritonclient[all]) (1.26.4)\n",
      "Collecting python-rapidjson>=0.9.1 (from tritonclient[all])\n",
      "  Downloading python_rapidjson-1.20-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: urllib3>=2.0.7 in /home/brad/miniforge3/envs/notebook_env/lib/python3.12/site-packages (from tritonclient[all]) (2.3.0)\n",
      "Collecting aiohttp<4.0.0,>=3.8.1 (from tritonclient[all])\n",
      "  Downloading aiohttp-3.11.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: cuda-python in /home/brad/miniforge3/envs/notebook_env/lib/python3.12/site-packages (from tritonclient[all]) (12.6.0)\n",
      "Collecting geventhttpclient>=2.3.3 (from tritonclient[all])\n",
      "  Downloading geventhttpclient-2.3.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting grpcio<1.68,>=1.63.0 (from tritonclient[all])\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: packaging>=14.1 in /home/brad/miniforge3/envs/notebook_env/lib/python3.12/site-packages (from tritonclient[all]) (24.2)\n",
      "Collecting protobuf<6.0dev,>=5.26.1 (from tritonclient[all])\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[all])\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.1->tritonclient[all])\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/brad/miniforge3/envs/notebook_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.1->tritonclient[all])\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.1->tritonclient[all])\n",
      "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[all])\n",
      "  Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.1->tritonclient[all])\n",
      "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting gevent (from geventhttpclient>=2.3.3->tritonclient[all])\n",
      "  Downloading gevent-24.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: certifi in /home/brad/miniforge3/envs/notebook_env/lib/python3.12/site-packages (from geventhttpclient>=2.3.3->tritonclient[all]) (2024.12.14)\n",
      "Requirement already satisfied: brotli in /home/brad/miniforge3/envs/notebook_env/lib/python3.12/site-packages (from geventhttpclient>=2.3.3->tritonclient[all]) (1.1.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/brad/miniforge3/envs/notebook_env/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (3.10)\n",
      "Collecting zope.event (from gevent->geventhttpclient>=2.3.3->tritonclient[all])\n",
      "  Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting zope.interface (from gevent->geventhttpclient>=2.3.3->tritonclient[all])\n",
      "  Downloading zope.interface-7.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Collecting greenlet>=3.1.1 (from gevent->geventhttpclient>=2.3.3->tritonclient[all])\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: setuptools in /home/brad/miniforge3/envs/notebook_env/lib/python3.12/site-packages (from zope.event->gevent->geventhttpclient>=2.3.3->tritonclient[all]) (75.8.0)\n",
      "Downloading aiohttp-3.11.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading geventhttpclient-2.3.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
      "Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading python_rapidjson-1.20-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tritonclient-2.54.0-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Downloading gevent-24.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading zope.interface-7.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "Installing collected packages: zope.interface, zope.event, python-rapidjson, protobuf, propcache, multidict, grpcio, greenlet, frozenlist, aiohappyeyeballs, yarl, tritonclient, gevent, aiosignal, geventhttpclient, aiohttp\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 frozenlist-1.5.0 gevent-24.11.1 geventhttpclient-2.3.3 greenlet-3.1.1 grpcio-1.67.1 multidict-6.1.0 propcache-0.2.1 protobuf-5.29.3 python-rapidjson-1.20 tritonclient-2.54.0 yarl-1.18.3 zope.event-5.0 zope.interface-7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient import utils as triton_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to False for remote/cloud deployment\n",
    "run_locally = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace HOST with the actual server URL where your Triton Inference Server is hosted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_locally:\n",
    "    HOST = 'localhost'\n",
    "else:\n",
    "    HOST = '<SERVER_URL>' # Replace with your server URL or IP address\n",
    "\n",
    "HTTP_PORT = 8000\n",
    "GRPC_PORT = 8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are testing a local deployment\n",
    "- Pull Triton inference server docker image\n",
    "- Deploy server with  models and configuration files (produced by the training NIM)\n",
    "- Double check that your model repository folder has the following structures\n",
    "```sh\n",
    "├── model\n",
    "│   ├── 1\n",
    "│   │   └── graph_sage_node_embedder.onnx\n",
    "│   └── config.pbtxt\n",
    "└── xgboost\n",
    "    ├── 1\n",
    "    │   └── xgboost_on_embeddings.json\n",
    "    └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post \"http://%2Fvar%2Frun%2Fdocker.sock/v1.47/images/create?fromImage=nvcr.io%2Fnvidia%2Ftritonserver&tag=25.01-py3\": dial unix /var/run/docker.sock: connect: permission denied\n",
      "permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post \"http://%2Fvar%2Frun%2Fdocker.sock/v1.47/containers/tritonserver/stop\": dial unix /var/run/docker.sock: connect: permission denied\n",
      "permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Delete \"http://%2Fvar%2Frun%2Fdocker.sock/v1.47/containers/tritonserver\": dial unix /var/run/docker.sock: connect: permission denied\n",
      "docker: permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Head \"http://%2Fvar%2Frun%2Fdocker.sock/_ping\": dial unix /var/run/docker.sock: connect: permission denied.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "if run_locally:\n",
    "    \n",
    "    # Triton server image\n",
    "    TRITON_IMAGE = 'nvcr.io/nvidia/tritonserver:25.01-py3'\n",
    "    MODEL_REPO_PATH = os.path.join(dataset_root_dir, 'trained_models/model_repository')\n",
    "\n",
    "    # Pull docker \n",
    "    !docker pull {TRITON_IMAGE}\n",
    "    !docker stop tritonserver\n",
    "    !docker rm tritonserver\n",
    "\n",
    "    !docker run --gpus all -d -p {HTTP_PORT}:{HTTP_PORT} -p {GRPC_PORT}:{GRPC_PORT} -v {MODEL_REPO_PATH}:/models --name tritonserver {TRITON_IMAGE} tritonserver --model-repository=/models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs for GRPC and HTTP request to the inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')\n",
    "client_http = httpclient.InferenceServerClient(url=f'{HOST}:{HTTP_PORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the triton inference server to come online\n",
    "NOTE: If the following cell keeps running longer then interrupt execution and run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m triton_utils\u001b[38;5;241m.\u001b[39mInferenceServerException:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "TIMEOUT = 60\n",
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')\n",
    "server_start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        if client_grpc.is_server_ready() or time.time() - server_start > TIMEOUT:\n",
    "            break\n",
    "    except triton_utils.InferenceServerException:\n",
    "        pass\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For local deployment, check if the triton inference server is running properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_locally:\n",
    "    !docker logs tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read preprocessed input transactions to make query to the triton inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_path = os.path.join(dataset_root_dir, \"xgb/test.csv\") # already preprocessed data\n",
    "test_df = pd.read_csv(test_path)\n",
    "X = test_df.iloc[:, :-1].values.astype(np.float32)\n",
    "y = test_df.iloc[:, -1].values\n",
    "edge_index = np.array([[], []]).astype(np.int64) # empty edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the HTTP request's inputs and output to retrieve embeddings for the input transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = httpclient.InferInput(\"x\", X.shape, datatype=\"FP32\")\n",
    "input_features.set_data_from_numpy(X)\n",
    "\n",
    "input_edge_indices = httpclient.InferInput(\"edge_index\", edge_index.shape, datatype=\"INT64\")\n",
    "input_edge_indices.set_data_from_numpy(edge_index)\n",
    "\n",
    "outputs = httpclient.InferRequestedOutput(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a query to retrieve embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the server\n",
    "results = client_http.infer(model_name=\"model\", inputs=[input_features, input_edge_indices], outputs=[outputs])\n",
    "node_embeddings = results.as_numpy('output')\n",
    "# print(node_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the retrieved embeddings as inputs to predict the transactions' fraud scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_input = httpclient.InferInput(\"input__0\", node_embeddings.shape, datatype=\"FP32\")\n",
    "xgboost_input.set_data_from_numpy(node_embeddings)\n",
    "\n",
    "xgboost_outputs = httpclient.InferRequestedOutput(\"output__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a query to retrieve the fraud scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client_http.infer(model_name=\"xgboost\", inputs=[xgboost_input], outputs=[xgboost_outputs])\n",
    "predictions = results.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision threshold to flag a transaction as fraud\n",
    "#Change to trade-off precision and recall\n",
    "decision_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = (predictions > decision_threshold).astype(int)\n",
    "\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred, zero_division=0)\n",
    "recall = recall_score(y, y_pred, zero_division=0)\n",
    "f1 = f1_score(y, y_pred, zero_division=0)\n",
    "\n",
    "print(\"----Summary---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create a DataFrame with labeled rows and columns\n",
    "classes = ['Non-Fraud', 'Fraud']\n",
    "columns = pd.MultiIndex.from_product([[\"Predicted\"], classes])\n",
    "index = pd.MultiIndex.from_product([[\"Actual\"], classes])\n",
    "\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "cm_df = pd.DataFrame(conf_mat, index=index, columns=columns)\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Plot the confusion matrix directly from predictions\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    y, y_pred, display_labels=classes)\n",
    "disp.ax_.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_env",
   "language": "python",
   "name": "notebook_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
